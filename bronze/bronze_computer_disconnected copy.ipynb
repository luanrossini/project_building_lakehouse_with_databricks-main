{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "from collections import OrderedDict ,Counter\n",
    "from functools import reduce \n",
    "from pyspark.sql.streaming import * \n",
    "from delta.tables import * \n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the path of DBFS\n",
    "dbutils.fs.ls(\"dbfs:/FileStore/landing_zone/Computer/Disconnected/Computer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input\n",
    "source_location = 'dbfs:/FileStore/landing_zone/Computer/Disconnected/Computer/'\n",
    "#Output\n",
    "target_location = 'dbfs:/FileStore/bronze/Computer/Disconnected/Computer/'\n",
    "target_table = 'spark_catalog.bronze.computer_disconnected' \n",
    "checkpoint = 'dbfs:/FileStore/bronze/Computer/Disconnected/Computer_chk/' # Define the directory of checkpoint\n",
    "schema = 'dbfs:/FileStore/bronze/Computer/Disconnected/Computer_schema/' # Define the Local schema\n",
    "source = 'Computer Disconnected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDF = (spark.readStream.format('cloudFiles')\\\n",
    "    .option('cloudFiles.Format', 'parquet')\\\n",
    "    .option('cloudFiles.inferColumnTypes', 'true')\\\n",
    "    .option('cloudFiles.schemaLocation', schema)\\\n",
    "    .option('cloudFiles.schemaEvolutionMode', 'addNewColumns')\\\n",
    "    .load(source_location)\\\n",
    "     # Add the path   \n",
    "     .withColumn('tracking_source',input_file_name())\\\n",
    "     # Add the source of information \n",
    "     .withColumn('source',lit(source)) \\\n",
    "     # Add the datetime from source data\n",
    "     .withColumn(\"ingestionin\", col(\"_metadata.file_modification_time\")) \\\n",
    "     # Add a new column called \"status\" if necessary  \n",
    "     .withColumn('status',lit(True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving on a table target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (streamingDF \n",
    "    .writeStream\n",
    "    # .queryName(\"spark_catalog.bronze.celular_conectado\")\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint)\n",
    "    .option(\"path\", target_location)\n",
    "    .trigger(availableNow=True)\n",
    "    .trigger(continuous='1 second') # Continuous process over a certain period\n",
    "    .table(target_table)\n",
    "    #.start()  #Finalizar o fluxo\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
