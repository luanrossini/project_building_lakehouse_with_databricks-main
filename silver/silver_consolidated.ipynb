{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Change Data Feed (CDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Enabling the CDF for all tables\n",
    "\n",
    "%sql\n",
    "set spark.databricks.delta.properties.defaults.enableChangeDataFeed = true;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data and use the streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer connected\n",
    "\n",
    "source_table = 'spark_catalog.bronze.computer_connected'\n",
    "df_readStream_computer_connected = (spark.readStream\n",
    "                                            .format(\"delta\")\n",
    "                                            .option(\"ignoreDeletes\", \"true\")\n",
    "                                            .table(source_table)\n",
    "                                            .withColumn(\"Classification\",when(col(\"Velocity\").between(0, 20), \"Low\")\n",
    "                                                                        .when(col(\"Velocity\").between(21, 100), \"Medium\")\n",
    "                                                                        .otherwise(\"Hight\"))\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer Disconnected\n",
    "\n",
    "source_table = 'spark_catalog.bronze.computer_disconnected'\n",
    "df_readStream_computer_disconnected = (spark.readStream\n",
    "                                            .format(\"delta\")\n",
    "                                            .option(\"ignoreDeletes\", \"true\")\n",
    "                                            .table(source_table)\n",
    "                                            .withColumn(\"Classification\",when(col(\"Velocity\").between(0, 20), \"Low\")\n",
    "                                                                        .when(col(\"Velocity\").between(21, 100), \"Medium\")\n",
    "                                                                        .otherwise(\"Hight\"))\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mobile connected\n",
    "\n",
    "source_table = 'spark_catalog.bronze.mobile_connected'\n",
    "df_readStream_mobile_connected = (spark.readStream\n",
    "                                            .format(\"delta\")\n",
    "                                            .option(\"ignoreDeletes\", \"true\")\n",
    "                                            .table(source_table)\n",
    "                                            .withColumn(\"Classification\",when(col(\"Velocity\").between(0, 20), \"Low\")\n",
    "                                                                        .when(col(\"Velocity\").between(21, 100), \"Medium\")\n",
    "                                                                        .otherwise(\"Hight\"))\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mobile Disconnected\n",
    "\n",
    "source_table = 'spark_catalog.bronze.mobile_disconnected'\n",
    "df_readStream_mobile_disconnected = (spark.readStream\n",
    "                                            .format(\"delta\")\n",
    "                                            .option(\"ignoreDeletes\", \"true\")\n",
    "                                            .table(source_table)\n",
    "                                            .withColumn(\"Classification\",when(col(\"Velocity\").between(0, 20), \"Low\")\n",
    "                                                                        .when(col(\"Velocity\").between(21, 100), \"Medium\")\n",
    "                                                                        .otherwise(\"Hight\"))\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unifying new batch dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fazendo o union all de todos os dataframes pelo nome da coluna\n",
    "dfs_individuais = [ df_readStream_computer_connected\n",
    "                   ,df_readStream_mobile_connected\n",
    "                   ,df_readStream_computer_disconnected\n",
    "                   ,df_readStream_mobile_disconnected]\n",
    "\n",
    "def union_all(dfs):\n",
    "    if len(dfs) > 1:\n",
    "        return dfs[0].unionByName(union_all(dfs[1:]), allowMissingColumns=True).distinct()\n",
    "    else:\n",
    "        return dfs[0].distinct()\n",
    "\n",
    "df_all = union_all(dfs_individuais)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recording on the silver layer with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_table = 'spark_catalog.silver.consolidated_connection'\n",
    "chekpoint = 'dbfs:/FileStore/silver/consolidated_connection'\n",
    "target_location = 'dbfs:/FileStore/silver/conexao_unificada'\n",
    "\n",
    "( df_all.writeStream\n",
    "  .format(\"delta\")\n",
    "  .outputMode(\"append\")\n",
    "  .option(\"checkpointLocation\", chekpoint)\n",
    "  .option(\"path\", target_location)\n",
    "  .trigger(availableNow=True)\n",
    "  .table(target_table)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
